{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math as math\n",
    "from numpy.typing import NDArray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nueron_layer2:\n",
    "    def __init__(self):\n",
    "        self.act_out = 0\n",
    "        self.lin_out = 0\n",
    "        self.weights = np.random.rand(9)\n",
    "    \n",
    "    def activation(self): # Implement Hard Sigmoid\n",
    "        return np.clip((self.lin_out + 1) / 2, 0, 1)\n",
    "    \n",
    "    def forward_pass(self, input: np.array):#size 8 \n",
    "        self.lin_out = np.dot(input, self.weights)\n",
    "        self.act_out = self.activation()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nueron_layer1:\n",
    "    def __init__(self):\n",
    "        self.act_out = 0\n",
    "        self.lin_out = 0\n",
    "        self.weights = np.random.rand(10)\n",
    "    \n",
    "    def activation(self): # Implement ReLu\n",
    "        return np.clip(self.lin_out, 0, None)\n",
    "    \n",
    "    def forward_pass(self, input: np.array):#size 1\n",
    "        self.lin_out = np.dot(input, self.weights)\n",
    "        self.act_out = self.activation()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def der_hard_sigmoid(x):\n",
    "    if x > 1 or x < -1:\n",
    "        return 0\n",
    "    elif x > -1 and x < 1:\n",
    "        return 1/2\n",
    "\n",
    "def der_ReLu(x):\n",
    "\n",
    "    if x > 0 :\n",
    "        return 1\n",
    "    elif x < 0:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.n2a = nueron_layer2()\n",
    "        self.n1 = [nueron_layer1() for i in range(8)]\n",
    "\n",
    "    def initalise_trainingdata(self, input:NDArray, labels, lr, niter):\n",
    "        self.n = len(input)\n",
    "        self.input = input\n",
    "        self.labels = labels\n",
    "        self.learning_rate = lr\n",
    "        self.niter = niter # number of iterations in one epoch\n",
    "\n",
    "    # input length 8\n",
    "    def ForwardPassNueralNet(self, index=0):\n",
    "        \n",
    "        assert len(self.input[index]) == len(self.n1)\n",
    "        \n",
    "        for i in range(len(self.input[index])):      \n",
    "            self.n1[i].forward_pass(np.array([self.input[index][i]]))\n",
    "        self.n2a.forward_pass(np.array([j.act_out for j in self.n1]))\n",
    "\n",
    "\n",
    "    def gradient_calc(self): # one epoch\n",
    "    \n",
    "    # Square Loss ~ (output[i] - label[i])**2 ~ Is The Loss Function \n",
    "\n",
    "        output = []\n",
    "        gradient_vector = np.zeros(10*8 + 9) # first 8 are parameters related to the only nueron in nueron layer 2\n",
    "                                    # last 8 are parameters related to the 8 nueron in nueron layer 1\n",
    "        for i in range(len(self.input)):\n",
    "            self.ForwardPassNueralNet(i)\n",
    "            output.append(self.n2a.act_out)\n",
    "            for param in range(9):\n",
    "                gradient_vector[param] += 2*(self.n2a.act_out - self.labels[i])*der_hard_sigmoid(self.n2a.lin_out)*self.n1[param].act_out\n",
    "            for param in range(8, 16):\n",
    "                gradient_vector[param] += 2*(self.n2a.act_out - self.labels[i])*der_hard_sigmoid(self.n2a.lin_out)*self.n2a.weights[param-8]*der_ReLu(self.n1[param-8].lin_out)*self.input[i][param - 8]\n",
    "            for i in range(len(self.n1)):\n",
    "                for param in range(10):\n",
    "        return gradient_vector\n",
    "    \n",
    "\n",
    "    def gradient_descent(self):\n",
    "\n",
    "\n",
    "        for _ in range(self.niter):\n",
    "            gradients = self.gradient_calc()\n",
    "            for i in range(len(self.n1)):\n",
    "                self.n1[i].weights -=  self.learning_rate*gradients[i+8]\n",
    "\n",
    "            for i in range(len(self.n2a.weights)):\n",
    "                self.n2a.weights -= self.learning_rate*gradients[i]\n",
    "\n",
    "    \n",
    "    def calc_loss(self):\n",
    "        output = []\n",
    "        # first 8 are parameters related to the only nueron in nueron layer 2\n",
    "        # last 8 are parameters related to the 8 nueron in nueron layer 1\n",
    "        \n",
    "        for i in range(len(self.input)):\n",
    "            self.ForwardPassNueralNet(i)\n",
    "            output.append(self.n2a.act_out)\n",
    "\n",
    "        return sum([(self.labels[i] - output[i])**2 for i in range(len(output))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2a = nueron_layer2()\n",
    "# be careful with the input\n",
    "n1 = [nueron_layer1() for i in range(8)]\n",
    "n = 1000 # number of datapoints in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input length 8\n",
    "def ForwardPassNueralNet(input: np.array, n1 :list[nueron_layer1], n2a: nueron_layer2):\n",
    "    \n",
    "    assert len(input) == len(n1)\n",
    "    \n",
    "    for i in range(len(input)):      \n",
    "        n1[i].forward_pass(np.array([input[i]]))\n",
    "\n",
    "    n2a.forward_pass(np.array([j.act_out for j in n1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "ForwardPassNueralNet([1,2,3,4,5,6,7,8], n1, n2a)\n",
    "print(n2a.act_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_calc(input: NDArray, label, n1:list[nueron_layer1], n2a:nueron_layer2): # one epoch\n",
    "    \n",
    "    # Square Loss ~ (output[i] - label[i])**2 ~ Is The Loss Function \n",
    "\n",
    "    output = []\n",
    "    gradient_vector = np.zeros(16) # first 8 are parameters related to the only nueron in nueron layer 2\n",
    "                                   # last 8 are parameters related to the 8 nueron in nueron layer 1\n",
    "    for i in range(len(input)):\n",
    "        ForwardPassNueralNet(input[i], n1, n2a)\n",
    "        output.append(n2a.act_out)\n",
    "        for param in range(8):\n",
    "            gradient_vector[param] += 2*(n2a.act_out - label[i])*der_hard_sigmoid(n2a.lin_out)*n1[param].act_out\n",
    "        for param in range(8, 16):\n",
    "            gradient_vector[param] += 2*(n2a.act_out - label[i])*der_hard_sigmoid(n2a.lin_out)*n2a.weights[param -8]*der_ReLu(n1[param-8].lin_out)*input[i][param - 8]\n",
    "    \n",
    "    return gradient_vector\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(input: NDArray, label, n1:list[nueron_layer1], n2a:nueron_layer2):\n",
    "\n",
    "    learning_rate = 0.1\n",
    "    number_of_iters = 1000\n",
    "\n",
    "    for _ in range(number_of_iters):\n",
    "        gradients = gradient_calc(input, label, n1, n2a)\n",
    "        for i in range(len(n1)):\n",
    "            n1[i].weights -=  learning_rate*gradients[i+8]\n",
    "\n",
    "        for i in range(len(n2a.weights)):\n",
    "            n2a.weights -= learning_rate*gradients[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(input: NDArray, label, n1:list[nueron_layer1], n2a:nueron_layer2):\n",
    "    output = []\n",
    "    # first 8 are parameters related to the only nueron in nueron layer 2\n",
    "    # last 8 are parameters related to the 8 nueron in nueron layer 1\n",
    "    \n",
    "    for i in range(len(input)):\n",
    "        ForwardPassNueralNet(input[i], n1, n2a)\n",
    "        output.append(n2a.act_out)\n",
    "\n",
    "    return sum([(label[i] - output[i])**2 for i in range(len(output))])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
